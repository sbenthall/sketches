{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frontier Notation\n",
    "\n",
    "This notebook considers the design and implementation of the next generation of heterogenous agent model (HAM) notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tikzmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "Unless otherwise stated, this analysis applies to additively separable problems only.\n",
    "\n",
    "### Bellman Equation\n",
    "\n",
    "A [Bellman equation](https://en.wikipedia.org/wiki/Bellman_equation) is a formulation of the value to an agent of each state acting optimally within a dynamic decision problem. From this value function it is easy to compute the optimal policy for the agent. It has been \"discovered\" and used in many fields, and goes by many names and notational variations across disciplines (Powell, 2007).\n",
    "\n",
    "\n",
    "The Bellman equation takes the form (**Definition 1**):\n",
    "\n",
    "$$V(x) = \\text{max}_{a \\in \\Gamma(x)} F(x,a) + \\beta V(T(x,a))$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $a \\in A$, the set of actions available\n",
    "* $x \\in X$, the set of states\n",
    "* $\\Gamma : X \\rightarrow \\mathscr{B}(A)$ constraints the actions available in state X\n",
    "* $F: X \\times A \\rightarrow \\mathbb{R}$ is the payoff to the agent of taking action $a$ in state $x$\n",
    "* $T: X \\times A \\rightarrow X$ is a transition function determining the state in the next period\n",
    "* $\\beta$ is a discount factor on future payoffs. **Maybe $\\beta: X \\times A \\rightarrow \\mathbb{R}$**\n",
    "\n",
    "This representation of Bellman equation assumes a deterministic model. If T is a stochastic function, $T_{i,j}: X_i \\times A_i \\rightarrow P(X_j)$, as it is in many cases in economics, then the value function is defined in terms of expectations. (**Definition 2**):\n",
    "\n",
    "$$V(x) = \\text{max}_{a \\in \\Gamma(x)} F(x,a) + E[\\beta  V(T(x,a))]$$\n",
    "\n",
    "We will use the deterministic version of this equation for simplicity unless there's a good reason to use the probabilistic version.\n",
    "\n",
    "**TODO: Define a policy $\\pi$**\n",
    "\n",
    "#### Time and finitude.\n",
    "\n",
    "Note that in the above definitions, there is only one state space, action space, value function, etc. that is repeated \"over time\". However, it is possible to model \"changes over time\" within this formalism by including an indicator of the \"time\" within the model in the state space. While these definitions are in this sense perfectly general, various desiderata have made alternative or reduced variations of this formalism desireable.\n",
    "\n",
    "Though perhaps a source of confusion, we will note that the time index is often used explicitly in the definition of the Bellman equation, i.e.:\n",
    "\n",
    "$$V_t(x) = \\text{max}_{a \\in \\Gamma(x)} F(x,a) + E[\\beta  V_{t+1}(T(x,a))]$$\n",
    "\n",
    "This is especially the case when the equation is more \"worked out\" and the state space is articulated in terms of model variables (see below).\n",
    "\n",
    "The explicit representation of time encourages the articulation of \"finite horizon\" models which have a notional maximal time limit. In theory, this is accomodated by the formalism above by stipulating that for all $\\hat{t} > K$, $V_{\\hat{t}} = 0$.\n",
    "\n",
    "As we will see, as we constrain the space of Bellman problems to classes of problems with certain shared features, we can develop general solution algorithms that are more efficient at solving the problems. This motivates a deeper investigation of Bellman equations and their use in, e.g., economics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the Model\n",
    "\n",
    "Some basic background material on solution algorithms for Bellman problems.\n",
    "For now this assumes discrete problem spaces, though we will ultimately be interested in continuous problem spaces.\n",
    "**For now, I will presume that the time complexity of continuous problem solutions is comparable to the time complexity of their discretizations. Eventually I will include a formal analysis of this elaborating on e.g. Carroll (2006).**\n",
    "\n",
    "#### Value Iteration\n",
    "\n",
    "Perhaps the simplest algorithm for solving a problem in Bellman form is **value iteration**.\n",
    "There are many varitions of value iteration. For the purposes of this preliminery analysis, we will examine one of the simplest.\n",
    "\n",
    "1. Initialize $V(x) \\leftarrow 0$ for all $x \\in X$. (This initialization can be to any arbitrary value for an infinite horizon problem.)\n",
    "2. Iteratively update until $\\forall x, |V(x) - V'(s)| < 0$:\n",
    "   1. $V' \\leftarrow V$\n",
    "   2. For all $x \\in X$:\n",
    "       1. For each $a \\in \\Gamma(x)$:\n",
    "          1. compute $Q(a,x) \\leftarrow F(x,a) + E[\\beta  V'(T(x,a))$\n",
    "       2. $V(x) \\leftarrow \\text{max}_{a \\in \\Gamma(x)} Q(a,x)$\n",
    "3. Output optimal policy $\\pi^*(x) \\leftarrow \\text{argmax}_{a \\in \\Gamma(x)} F(x,a) + E[\\beta  V(T(x,a)) $\n",
    "\n",
    "The computationally expensive part of value iteration is the update step (2.2). As written, the time complexity of one update step is $O(|X|^2|A|)$: a loop over the state space, a loop over the action space, and a final loop over the state space again in (2.2.1.1) to computer the expectated value from the next period.\n",
    "\n",
    "However, it should be noted here that the algorithm implementation can take advantage of the sparsity of the problem. Not every combination of $(x, a, x')$ need be explored at each update step. Let $X'(x) = {x' |\\exists a \\in A, T(x,a)(x') > 0}$, the set of states in the next period that have a positive probability under the distribution $T(x,a)$. Then the complexity of the update step (2.2) for a given $x$ is reduced to $O(|\\Gamma(x)||X'(x)|)$. Moreover, after a single pass ofr the state space at (2), it is possible to determine if some of the notional elements of $X$ are in fact never reached, and these can be excluded from future iterations.\n",
    "\n",
    "An example of when a transition function will be sparse is when the \"state\" includes a variable that tracks the \"time period\" of the problem, $t$. As this model variable increments once per period with $T$, it adds no \"width\" to the problem at (2.2.1.1), and so can be added or removed without increasing the time complexity of the solution. In fact, the inclusion of this \"variable\" is often elevated to a role in primary notation, distinguishing different iterations of the problem's updating or simulation function, or as an alternative to the \"prime\" notation. As the use of this \"time\" notation has caused confusion in the past, where possible we will avoid it in these architecture notes.\n",
    "\n",
    "#### Backwards induction\n",
    "\n",
    "Backwards induction is in many respects even simpler than value iteration. In finite cases, the value function at each step can be computed directly starting with the terminal case. This looks much like value iteration as described above except that:\n",
    " * whereas in value iteration the initial $V$ is arbitrary, in the backwards induction case $V_K$ is given by the model definition.\n",
    " * whereas in value iteration the intermediary computed value functions are discarded, in backwards they are stored as $V_t$ is the unique solution at the $t$ time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Equations and Structural Equation Modeling\n",
    "\n",
    "In practice, economists rarely describe a model directly in terms of the formalism of Definition 1. Rather, they use a technique that can be considered a special case of Structural Equation Modeling to define the elements of the model. Consider the following simple example:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "V_t(M_t,P_t) &=& \\max_{C_t} U(C_t) + \\beta (1 - D_{t+1}) V_{t+1}(M_{t+1},P_{t+1}), \\\\\n",
    "& s.t. & \\\\\n",
    "A_t &=& M_t - C_t, \\\\\n",
    "A_t/P_t &\\geq& \\hat{a}, \\\\\n",
    "M_{t+1} &=& R A_t + Y_{t+1}, \\\\\n",
    "Y_{t+1} &=& P_{t+1}, \\\\\n",
    "P_{t+1} &=& G_{t+1} P_t.\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equations below the value function definition (a Bellman equation) are called _transition equations_. These are a concise representation of the problem that includes information that is relevant to the transition function $T$ and constraint function $\\Gamma$.\n",
    "\n",
    "From these equations it is possible to reconstruct the problem in the form used in (Definition 1). This is a relatively simple example.\n",
    "\n",
    "* $A = \\mathbb{R}$ corresponds to $C$\n",
    "* $X = \\mathbb{R} \\times \\mathbb{R}$ corresponds to $(M, P)$\n",
    "* $\\Gamma((M, P)) = \\{C | (M - C) / P \\geq \\hat{a}\\}$\n",
    "* $F((M,P),C) = U(C)$\n",
    "* $T((M, P), C) = (R(M - C) + GP, GP)$\n",
    "* $\\beta$ is constant\n",
    "\n",
    "We make several observations. First, the original transition equations are defined in terms of a set of variables. We will call these variables _model variables_. A model variable is not the same thing as an element of the Bellman problem; for example, the state space $X$ has two dimensions that correspond to the two variables $M$ and $P$.\n",
    "\n",
    "It is conventional in economics to distinguish between _control variables_ and _state variables_. Here $C$ is a control variable because it is notionally chosen by the agent to maximize the value of the value function $V$. Control variables are thus related to the action space $A$ but not identical to it. For example, it is possible to have more than one control variable.\n",
    "\n",
    "Note that the transition equations contains symbols (e.g., $Y$, $A$) that are not used in the definitions here. We will call these _state variables_ even though they do not show up in the state space $X$! **Do we need another name for this?**\n",
    "\n",
    "This is a good reminder that _transition equations_ here are not the same thing as the _transition function_ $T$ of the Bellman problem, though they have similar names and are clearly connected. This is the source of some confusion. **We will clarify this relationship in later sections of this document.**\n",
    "\n",
    "Note also that the substate $P$ never depends on the choice of an action. It grows at a rate of $G$ every period. We call variables that do not depend on an agent's choice _exogenous_ variables. **We will define this more carefully later in this document.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Transition Equations\n",
    "\n",
    "The above example is deterministic and contains no stochastic shocks. The following problem introduces stochastic shocks to income and normalizes the problem.\n",
    "\n",
    "The consumer receives two income shocks at the beginning of each period: a completely transitory shock $\\theta$ and a completely permanent shock $\\psi$.  Moreover, the agent is subject to borrowing a borrowing limit: the ratio of end-of-period assets $A_t$ to permanent income $P_t$ must be greater than $\\hat{a}$.  This model is stated in terms of *normalized* variables, dividing all real variables by $P_t$:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "v_t(m_t) &=& \\max_{c_t} u(c_t) + \\beta(1-D_{t+1})  \\mathbb{E}_{t} \\left[ (G_{t+1}\\psi_{t+1})^{1-\\rho} v_{t+1}(m_{t+1}) \\right], \\\\\n",
    "a_t &=& m_t - c_t, \\\\\n",
    "a_t &\\geq& \\text{$\\hat{a}$}, \\\\\n",
    "m_{t+1} &=& R/(G_{t+1} \\psi_{t+1}) a_t + \\theta_{t+1}, \\\\\n",
    "(\\psi_{t+1},\\theta_{t+1}) &\\sim& F_{t+1}, \\\\\n",
    "\\mathbb{E}[\\psi]=\\mathbb{E}[\\theta] &=& 1, \\\\\n",
    "u(c) &=& \\frac{c^{1-\\rho}}{1-\\rho}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "**TODO**: What is $F$ in the above equations?\n",
    "\n",
    "It is worthwhile to note that normalization by the income level has reduced the state space of the problem from two dimensions to one. This is believed to reduce the time complexity of solving the problem, though it is possible that a well-defined solution algorithm could exploit the sparseness and exogeneity of the income transition function and achieve comparable performance with more generality.\n",
    "\n",
    "However, what we are drawing attention to here is the use of stochastic _shocks_ $\\theta$ and $\\psi$ as model variables, which renders the entire problem probabilistic.\n",
    "\n",
    "* $A = \\mathbb{R}$ corresponds to $c$\n",
    "* $X = \\mathbb{R}$ corresponds to $m$\n",
    "* $\\Gamma(m) = \\{c | m - c \\geq \\hat{a}\\}$\n",
    "* $F(m,c) = u(c)$\n",
    "* $T(m, c) = R / (G \\psi) (m - c) + \\theta$\n",
    "* $\\beta$ is constant\n",
    "\n",
    "Note the transition function $T$ is a stochastic function of the form $T: X \\times A \\rightarrow P(X)$. This function implies the existence of a conditional probability distribution $P_T(m' | m, c) = P_T(m' | m, c, \\psi) P(\\psi)$ because of the independence of $\\psi$. The value function equation can be rewritten in terms of this distribution:\n",
    "\n",
    "$$v(m) = \\max_{c} u(c) + \\beta(1-D) \\mathbb{E}_{P_T} \\left[ (G \\psi)^{1-\\rho} v'(m') \\right]$$\n",
    "\n",
    "Viewing the transition function in terms of its implied probability distribution over model variables will help us determine the general conditions for solution techniques to Bellman problems.\n",
    "\n",
    "#### Graphical representation\n",
    "\n",
    "**Draw on Pearl.**\n",
    "\n",
    "**Model using Koller and Milch's conventions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variations\n",
    "\n",
    "The above formulation of the Bellman equation assumes that the problem is structured identically at every control point (opportunity for the agent to choose an action $a$) and that there is only one control point. For various applications in economics, we are interested in other kinds of problems, such as those with:\n",
    "\n",
    "* **V1**. Sequential control points\n",
    "* **V2**. Finite Problems Changing Over Time\n",
    "\n",
    "While it may be tempting to consider these the same kind of challenge, we will examine each case rigorously in isolation and see if that conclusion can be confirmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V1. Sequential control points\n",
    "\n",
    "**TODO: Under what conditions can a problem with multiple control variables be decomposed?!?!**\n",
    "\n",
    "There are many Bellman problems with multiple \"control variables\". According to the formalism in **Definition 1**, this means that the action space $A$ has multiple dimensions. In the SEM framework, this entails multiple model variables defined to be controls.\n",
    "\n",
    "Under some conditions (**TODO: What conditions?**) it is possible to decompose these problems into multiple subproblems with distinct value functions, state spaces, contraints, and transition functions. We will call this a \"sequential control points\" problem, or **V1** (Variation 1). \n",
    "\n",
    "For reasons connected to those described in \"Mathematics of Discrete Time Stochastic Control Processes in Macroeconomics\", problems of the following form, for $C$ (sequential) control variables, can be shown to have an equivalent problem in the above form: (Note that the variables here are in italic font)\n",
    "\n",
    "* $(\\mathit{A}_0, ..., \\mathit{A}_C)$ is a sequence of sets of action spaces such that $A = \\mathit{A}_0 \\times ... \\times \\mathit{A}_c$\n",
    "* $(\\mathit{X}_0, ...,\\mathit{X}_C)$ is a sequence of sets of state spaces such that $X = \\mathit{X}_0 \\times ... \\times \\mathit{X}_c$\n",
    "* $(\\mathit{\\Gamma}_0, ..., \\mathit{\\Gamma}_C)$ is a sequence of contraints. $\\mathit{\\Gamma}_c: \\mathit{X}_c \\rightarrow \\mathscr{B}(\\mathit{A}_c)$\n",
    "* $(\\mathit{F}_0, ..., \\mathit{F}_C)$ is a sequence reward functions. $\\mathit{F}_c: \\mathit{X}_c \\times \\mathit{A}_c \\rightarrow \\mathbb{R}$\n",
    "* $(\\mathit{T}_0, ..., \\mathit{T}_C)$ is a sequence of transitions functions $\\mathit{T}_c: \\mathit{X}_c \\times \\mathit{A}_c \\rightarrow \\mathit{X}_{(c+1) mod C}$. **Note incrementing of state space in transition function.**\n",
    "* $(\\mathit{\\beta}_0, ..., \\mathit{\\beta}_C)$ a potentially varying discount factor\n",
    "\n",
    "With these sequences of subproblems defined, the decomposed into $C$ mutually recursive Bellman-like *Carroll equations* (**Definition 3**) for $x \\in \\mathit{X}_c$ is:\n",
    "\n",
    "$$\\mathit{V}_c(x) = \\text{max}_{a \\in \\mathit{\\Gamma}_c(x)} \\mathit{F}_c(x,a) + \\mathit{\\beta}_c \\mathit{V}_{(c+1) mod C}(\\mathit{T}_c(x,a))$$\n",
    "\n",
    "In principle, the original problem could be defined in such a way that the sub-problems need not have a natural sequential order. However, in these cases multiple ordering will suffice.\n",
    "\n",
    "Note that these subproblems are not isolated from each other. The transition function $\\mathit{T}_c$ of each problem contains a \"reference\" to the state space in the next \"stage\" $\\mathit{X}_{c+1}$.\n",
    "\n",
    "These truly are \"subproblems\" of the original problem specification, as the original specification can be reconstituted from the details of the subproblems. For example, $A = \\mathit{A}_0 \\times ... \\times \\mathit{A}_c$, $X = \\mathit{X}_0 \\times ... \\times \\mathit{X}_c$ and so on. The problem remains an infinite horizon problem, but decomposed into $C$ simpler sub-problems with reduced action and state spaces.\n",
    "\n",
    "##### Example: Portfolio consumption problem\n",
    "\n",
    "**Unfinished. Requires translation into the above formalism.**\n",
    "\n",
    "Consider this version of the portfolio choice consumption saving problem.\n",
    "\n",
    "**TODO**: Complete this table with the elements from Definition 3.\n",
    "\n",
    "**PROBLEM**: What about $p$ in this formulation? Is it a state variable? Should it be added to the state space?\n",
    "\n",
    "Variable | Equation | Operation | Stage | Utility\n",
    "-- | -- | -- | -- | --\n",
    "\\Risky | ~Dist | Shock | c | --\n",
    "\\TranShk | ~Dist | Shock | c | --\n",
    "\\PermShk | ~Dist | Shock | c | --\n",
    "\\Rport | \\Rport = \\PortShare * \\Risky + (1 - \\PortShare) * R | Update | c | --\n",
    "$b$ | b_{t} = a_{t-1} \\RPort | Update | c | --\n",
    "$p$ | p_{t}=p_{t-1}\\PermShk_{t} | Update | c | --\n",
    "$y$ | y_{t} = p_{t}\\TranShk_{t} | Update | c | --\n",
    "$m$ | m_{t} = b_{t} + y_{t} | Update | c | --\n",
    "$c$ | c | Control | c | U(c)\n",
    "$a$ | a_{t} = m_{t} - c_{t} | Update | $\\alpha$ | --\n",
    "$\\alpha$| $\\alpha$ | Control | $\\alpha$ | 0\n",
    "\n",
    "This can be decomposed into two subproblems (\"stages\"). The consumption stage:\n",
    "\n",
    "* $A_c = \\mathbb{R}$ corresponds to $c$\n",
    "* $X_c = \\mathbb{R}$ corresponds to $m$\n",
    "* $\\Gamma_c$ ... restrictions consumption $c \\leq m$\n",
    "* $F_c(x,a) = CRRA(a)$\n",
    "* $T_c(x,a) = ... $ ...\n",
    "* $\\beta_c = 1 $\n",
    "\n",
    "The allocation stage:\n",
    "\n",
    "* $A_\\alpha = \\mathbb{R}$ corresponding to $\\alpha$\n",
    "* $X_\\alpha = \\mathbb{R} $ corresponding to $a$\n",
    "* $\\Gamma_\\alpha$ ... restrictions consumption $0 \\leq \\alpha \\leq 1$\n",
    "* $F_\\alpha(x,a) = 0$\n",
    "* $T_\\alpha(x,a) = ... $ ...\n",
    "* $\\beta_\\alpha = \\beta $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"400px\" height=\"240px\" viewBox=\"0 0 73.274 37.264\" version=\"1.1\">\n",
       "<defs>\n",
       "<g>\n",
       "<symbol overflow=\"visible\" id=\"glyph0-0\">\n",
       "<path style=\"stroke:none;\" d=\"\"/>\n",
       "</symbol>\n",
       "<symbol overflow=\"visible\" id=\"glyph0-1\">\n",
       "<path style=\"stroke:none;\" d=\"M 3.953125 -3.78125 C 3.78125 -3.78125 3.65625 -3.78125 3.515625 -3.65625 C 3.34375 -3.5 3.328125 -3.328125 3.328125 -3.265625 C 3.328125 -3.015625 3.515625 -2.90625 3.703125 -2.90625 C 3.984375 -2.90625 4.25 -3.15625 4.25 -3.546875 C 4.25 -4.03125 3.78125 -4.40625 3.078125 -4.40625 C 1.734375 -4.40625 0.40625 -2.984375 0.40625 -1.578125 C 0.40625 -0.671875 0.984375 0.109375 2.03125 0.109375 C 3.453125 0.109375 4.28125 -0.953125 4.28125 -1.0625 C 4.28125 -1.125 4.234375 -1.203125 4.171875 -1.203125 C 4.109375 -1.203125 4.09375 -1.171875 4.03125 -1.09375 C 3.25 -0.109375 2.15625 -0.109375 2.046875 -0.109375 C 1.421875 -0.109375 1.140625 -0.59375 1.140625 -1.203125 C 1.140625 -1.609375 1.34375 -2.578125 1.6875 -3.1875 C 2 -3.765625 2.546875 -4.1875 3.09375 -4.1875 C 3.421875 -4.1875 3.8125 -4.0625 3.953125 -3.78125 Z M 3.953125 -3.78125 \"/>\n",
       "</symbol>\n",
       "<symbol overflow=\"visible\" id=\"glyph0-2\">\n",
       "<path style=\"stroke:none;\" d=\"M 4.75 -2.359375 C 4.75 -3.921875 3.828125 -4.40625 3.09375 -4.40625 C 1.71875 -4.40625 0.40625 -2.984375 0.40625 -1.578125 C 0.40625 -0.640625 1 0.109375 2.03125 0.109375 C 2.65625 0.109375 3.375 -0.125 4.125 -0.734375 C 4.25 -0.203125 4.578125 0.109375 5.03125 0.109375 C 5.5625 0.109375 5.875 -0.4375 5.875 -0.59375 C 5.875 -0.671875 5.8125 -0.703125 5.75 -0.703125 C 5.6875 -0.703125 5.65625 -0.671875 5.625 -0.59375 C 5.4375 -0.109375 5.078125 -0.109375 5.0625 -0.109375 C 4.75 -0.109375 4.75 -0.890625 4.75 -1.125 C 4.75 -1.328125 4.75 -1.359375 4.859375 -1.46875 C 5.796875 -2.65625 6 -3.8125 6 -3.8125 C 6 -3.84375 5.984375 -3.921875 5.875 -3.921875 C 5.78125 -3.921875 5.78125 -3.890625 5.734375 -3.703125 C 5.546875 -3.078125 5.21875 -2.328125 4.75 -1.734375 Z M 4.09375 -0.984375 C 3.203125 -0.21875 2.4375 -0.109375 2.046875 -0.109375 C 1.453125 -0.109375 1.140625 -0.5625 1.140625 -1.203125 C 1.140625 -1.6875 1.40625 -2.765625 1.71875 -3.265625 C 2.1875 -4 2.734375 -4.1875 3.078125 -4.1875 C 4.0625 -4.1875 4.0625 -2.875 4.0625 -2.109375 C 4.0625 -1.734375 4.0625 -1.15625 4.09375 -0.984375 Z M 4.09375 -0.984375 \"/>\n",
       "</symbol>\n",
       "</g>\n",
       "<clipPath id=\"clip1\">\n",
       "  <path d=\"M 50 4 L 73.273438 4 L 73.273438 33 L 50 33 Z M 50 4 \"/>\n",
       "</clipPath>\n",
       "<clipPath id=\"clip2\">\n",
       "  <path d=\"M 8 18 L 65 18 L 65 37.265625 L 8 37.265625 Z M 8 18 \"/>\n",
       "</clipPath>\n",
       "</defs>\n",
       "<g id=\"surface1\">\n",
       "<path style=\"fill:none;stroke-width:0.3985;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 7.703469 -0.0008125 C 7.703469 4.253094 4.25425 7.702313 0.00034375 7.702313 C -4.253563 7.702313 -7.702781 4.253094 -7.702781 -0.0008125 C -7.702781 -4.254719 -4.253563 -7.703937 0.00034375 -7.703937 C 4.25425 -7.703937 7.703469 -4.254719 7.703469 -0.0008125 Z M 7.703469 -0.0008125 \" transform=\"matrix(1,0,0,-1,7.902,18.632)\"/>\n",
       "<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n",
       "  <use xlink:href=\"#glyph0-1\" x=\"5.746\" y=\"20.777\"/>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1)\" clip-rule=\"nonzero\">\n",
       "<path style=\"fill:none;stroke-width:0.3985;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 65.172219 -0.0008125 C 65.172219 4.682781 61.375344 8.479656 56.69175 8.479656 C 52.012062 8.479656 48.215187 4.682781 48.215187 -0.0008125 C 48.215187 -4.684406 52.012062 -8.481281 56.69175 -8.481281 C 61.375344 -8.481281 65.172219 -4.684406 65.172219 -0.0008125 Z M 65.172219 -0.0008125 \" transform=\"matrix(1,0,0,-1,7.902,18.632)\"/>\n",
       "</g>\n",
       "<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n",
       "  <use xlink:href=\"#glyph0-2\" x=\"61.39\" y=\"20.777\"/>\n",
       "</g>\n",
       "<path style=\"fill:none;stroke-width:0.3985;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 5.586281 5.589031 C 17.883156 18.186688 37.957375 18.432781 50.226906 6.456219 \" transform=\"matrix(1,0,0,-1,7.902,18.632)\"/>\n",
       "<path style=\"fill:none;stroke-width:0.31879;stroke-linecap:round;stroke-linejoin:round;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -1.196895 1.593927 C -1.096775 0.995056 -0.000497247 0.100376 0.298488 0.000340274 C -0.000191255 -0.100681 -1.096234 -0.996207 -1.194384 -1.592664 \" transform=\"matrix(0.71779,0.70052,0.70052,-0.71779,58.12926,12.17396)\"/>\n",
       "<g clip-path=\"url(#clip2)\" clip-rule=\"nonzero\">\n",
       "<path style=\"fill:none;stroke-width:0.3985;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 50.555031 -6.137531 C 37.957375 -18.434406 17.883156 -18.188312 5.9105 -5.914875 \" transform=\"matrix(1,0,0,-1,7.902,18.632)\"/>\n",
       "</g>\n",
       "<path style=\"fill:none;stroke-width:0.31879;stroke-linecap:round;stroke-linejoin:round;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -1.195239 1.594397 C -1.094301 0.99522 0.00174119 0.0996935 0.297633 0.00139259 C -0.00135209 -0.0986435 -1.094842 -0.996043 -1.194962 -1.594915 \" transform=\"matrix(-0.70052,-0.71779,-0.71779,0.70052,13.81106,24.5486)\"/>\n",
       "</g>\n",
       "</svg>"
      ]
     },
     "metadata": {
      "isolated": "true"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%tikz -f svg\n",
    "\n",
    "\\usetikzlibrary{shapes}\n",
    "\n",
    "\\node[shape=circle,draw=black] (c) at (0,0) {$c$};\n",
    "\\node[shape=circle,draw=black] (alpha) at (2,0) {$\\alpha$};\n",
    "\n",
    "\\path [->] (c) edge [bend left=45] node[left] {} (alpha);\n",
    "\\path [->] (alpha) edge [bend left=45] node[right] {} (c);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Time discounting is after the $\\alpha$ step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V2. Finite Problems Changing Over Time\n",
    "\n",
    "There is a sense in which the challenge of changing over time is similar to the challenge of multiple control points. We will see that the formal descriptions of the challenges are similar. However, there are some differences:\n",
    "\n",
    "1. Whereas multiple control points allow us to break a problem down into $C$ subproblems with smaller state and action spaces than the original problem, inconsistency over time makes a problem *more* complex because it *increases* the size of the problem and set of problems that can be represented.\n",
    "2. One of these things will hold for any actually specified inconsistent problem: (a) the inconsistent problem is a finite problem, or (b) the inconsistent problem can be rewritten as a consistent problem with time index in its state space.\n",
    "\n",
    "We will formally define the time inconsistent problem in terms of the following, assuming for now that it is a finite problem:\n",
    "\n",
    "* $A_0, ..., A_K$ is a sequence of sets of action spaces\n",
    "* $X_0, ...,X_K$ is a sequence of sets of state spaces\n",
    "* $\\Gamma_0, ..., \\Gamma_C$ is a sequence of contraints. $\\Gamma_k: X_k \\rightarrow \\mathscr{B}(A_k)$\n",
    "* $F_0, ..., F_K$ is a sequence reward functions. $F_k: X_k \\times A_k \\rightarrow \\mathbb{R}$\n",
    "* $T_0, ..., T_K$ is a sequence of transitions functions $T_k: X_k \\times A_k \\rightarrow X_{k+1}$, with $T_K$ undefined. (see below for value function construction).\n",
    "* $\\beta_0 , ..., \\beta_K$, a (potentially varying) discount factor for utility between time steps \n",
    "\n",
    "Specific changes to the formalism include:\n",
    "\n",
    "* *Substance*: Whereas in V1 the problems are defined by mutual recursive with $\\mathit{T_C}: \\mathit{X}_C \\times \\mathit{A}_C \\rightarrow \\mathit{X}_0$, in this case $T_K$ is terminal.\n",
    "* *Notation*: Whereas in V1 the subproblem components were written in a different font `mathit` (**perhaps too subtle a difference; could use mathcal instead?**), these subproblems are not a valid decomposition of single, more complex infinite horizon problem and so use the standard font.\n",
    "\n",
    "\n",
    "With these sequences of subproblems defined, the Bellman-like *Carroll equations* are:\n",
    "\n",
    "$$V_K(x) = \\text{max}_{a \\in \\Gamma_K(x)} F_K(x,a)$$\n",
    "$$V_k(x) = \\text{max}_{a \\in \\Gamma_k(x)} F_k(x,a) + \\beta_k V_{k+1}(T_k(x,a))$$\n",
    "\n",
    "Because this is a finite model, this can be solved simply via backwards induction without looking for a convergent value of $V_k$. Note that as before, the transition functions $T_k$ link the problem at $k$ to the state space of the next time $X_{k+1}$.\n",
    "\n",
    "If the problem for any $k$ carries as much information as one problem in the mode of **Definition 1**, then the time inconsistent problem will have a longer minimum description lengtht than the original infinite horizon problem. This is in contrast to the decomposition of the infinite horizon problem in to multiple stages based on multiple control variables; in the latter case, the decomposition contains no new information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carroll Problems: a proposed unified formalism\n",
    "   \n",
    "The two variations on the classic Bellman equation above have clear formal similarities, though they have some important differences as well. For the sake of clarity, we will formally define a *Carroll Problem* in a way that both kinds of variation, and others besides it, can be represented easily.\n",
    "\n",
    "#### Deterministic case\n",
    "\n",
    "A Carroll Problem $\\mathbb{C}$ is a set of subproblems $\\{\\mathcal{C}_0, ... \\mathcal{C}_I\\}$.\n",
    "\n",
    "Each subproblem $\\mathcal{C}_i$ is a tuple of the following:\n",
    "\n",
    "* A definition of the local problem:\n",
    "    * $A_i$, the set of actions available\n",
    "    * $X_i$, the set of states\n",
    "    * $\\Gamma_i : X \\rightarrow \\mathscr{B}(A)$ constraints the actions available in state X\n",
    "    * $F_i: X \\times A \\rightarrow \\mathbb{R}$ is the payoff to the agent of taking action $a$ in state $x$\n",
    "* A information about the transition to the next problem. This could be:\n",
    "    * Null, in which case the problem is terminal, or ...\n",
    "    * A tuple containing:\n",
    "        * An index to another subproblem $j$\n",
    "        * $T_i: X_i \\times A_i \\rightarrow X_j$, a transition function determining the state in the next problem\n",
    "        * $\\beta_i$ is a discount factor on future payoffs.\n",
    "\n",
    "The Carroll problem (**Definition 6**) can then be solved using backwards induction by computing the value functions $V_i : X_i \\rightarrow \\mathbb{R}$:\n",
    "\n",
    "$$V_i(x) = \\text{max}_{a \\in \\Gamma_i(x)} F_i(x,a) + \\beta_i V_{j}(T_i(x,a))$$\n",
    "\n",
    "or for the terminal subproblems\n",
    "\n",
    "$$V_i(x) = \\text{max}_{a \\in \\Gamma_i(x)} F_i(x,a)$$\n",
    "\n",
    "Note that the connection between subproblems can be represented as a directed graph in the case of repetitions.\n",
    "\n",
    "**Quandary**. How does one represent an infinite horizon problem with a \"terminal solution\" ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic transitions\n",
    "\n",
    "As discussed above, by extending only the *transition* aspect of the problem, we can introduce stochasticity.\n",
    "Here, we introduce not only stochasticity not only within the state space of the \"next problem\", but also stochasticity between the indices of the next problem.\n",
    "\n",
    "Each subproblem $\\mathcal{C}_i$ is a tuple of the following:\n",
    "\n",
    "* A definition of the local problem:\n",
    "    * $A_i$, the set of actions available\n",
    "    * $X_i$, the set of states\n",
    "    * $\\Gamma_i : X \\rightarrow \\mathscr{B}(A)$ constraints the actions available in state X\n",
    "    * $F_i: X \\times A \\rightarrow \\mathbb{R}$ is the payoff to the agent of taking action $a$ in state $x$\n",
    "* A information about the transition to the next problem. This could be:\n",
    "    * Null, in which case the problem is terminal, or ...\n",
    "    * A tuple containing:\n",
    "        * $P_J$, a probability distribution over indices to other subproblems $j$\n",
    "        * $T_{i,j}: X_i \\times A_i \\rightarrow P(X_j)$, a transition function determining the state in the next problem. Note the output of the transition function is a probability distribution over states $X_j$ given problem $j$.\n",
    "        * $\\beta_i$ is a discount factor on future payoffs. **beta as a function of X and A**?\n",
    "        \n",
    "The value function (**Definition 8**) is then:\n",
    "\n",
    "$$V_i(x) = \\text{max}_{a \\in \\Gamma_i(x)} F_i(x,a) + E_{P_J}[\\beta_i V_{i,j}(T_{i,j}(x,a))]$$\n",
    "\n",
    "If we wanted to be extra fancy we could let $P_J: X \\times A \\rightarrow P(J)$ vary with state and action choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example: Consumption with mortality\n",
    "\n",
    "The infinite horizon consumption saving problem with a .02 mortality rate.\n",
    "\n",
    "The problem $d$ is terminal and allows no actions or utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"400px\" height=\"240px\" viewBox=\"0 0 93.486 32.882\" version=\"1.1\">\n",
       "<defs>\n",
       "<g>\n",
       "<symbol overflow=\"visible\" id=\"glyph0-0\">\n",
       "<path style=\"stroke:none;\" d=\"\"/>\n",
       "</symbol>\n",
       "<symbol overflow=\"visible\" id=\"glyph0-1\">\n",
       "<path style=\"stroke:none;\" d=\"M 3.953125 -3.78125 C 3.78125 -3.78125 3.65625 -3.78125 3.515625 -3.65625 C 3.34375 -3.5 3.328125 -3.328125 3.328125 -3.265625 C 3.328125 -3.015625 3.515625 -2.90625 3.703125 -2.90625 C 3.984375 -2.90625 4.25 -3.15625 4.25 -3.546875 C 4.25 -4.03125 3.78125 -4.40625 3.078125 -4.40625 C 1.734375 -4.40625 0.40625 -2.984375 0.40625 -1.578125 C 0.40625 -0.671875 0.984375 0.109375 2.03125 0.109375 C 3.453125 0.109375 4.28125 -0.953125 4.28125 -1.0625 C 4.28125 -1.125 4.234375 -1.203125 4.171875 -1.203125 C 4.109375 -1.203125 4.09375 -1.171875 4.03125 -1.09375 C 3.25 -0.109375 2.15625 -0.109375 2.046875 -0.109375 C 1.421875 -0.109375 1.140625 -0.59375 1.140625 -1.203125 C 1.140625 -1.609375 1.34375 -2.578125 1.6875 -3.1875 C 2 -3.765625 2.546875 -4.1875 3.09375 -4.1875 C 3.421875 -4.1875 3.8125 -4.0625 3.953125 -3.78125 Z M 3.953125 -3.78125 \"/>\n",
       "</symbol>\n",
       "<symbol overflow=\"visible\" id=\"glyph0-2\">\n",
       "<path style=\"stroke:none;\" d=\"M 5.140625 -6.8125 C 5.140625 -6.8125 5.140625 -6.921875 5.015625 -6.921875 C 4.859375 -6.921875 3.921875 -6.828125 3.75 -6.8125 C 3.671875 -6.796875 3.609375 -6.75 3.609375 -6.625 C 3.609375 -6.5 3.703125 -6.5 3.84375 -6.5 C 4.328125 -6.5 4.34375 -6.4375 4.34375 -6.328125 L 4.3125 -6.125 L 3.71875 -3.765625 C 3.53125 -4.140625 3.25 -4.40625 2.796875 -4.40625 C 1.640625 -4.40625 0.40625 -2.9375 0.40625 -1.484375 C 0.40625 -0.546875 0.953125 0.109375 1.71875 0.109375 C 1.921875 0.109375 2.421875 0.0625 3.015625 -0.640625 C 3.09375 -0.21875 3.453125 0.109375 3.921875 0.109375 C 4.28125 0.109375 4.5 -0.125 4.671875 -0.4375 C 4.828125 -0.796875 4.96875 -1.40625 4.96875 -1.421875 C 4.96875 -1.53125 4.875 -1.53125 4.84375 -1.53125 C 4.75 -1.53125 4.734375 -1.484375 4.703125 -1.34375 C 4.53125 -0.703125 4.359375 -0.109375 3.953125 -0.109375 C 3.671875 -0.109375 3.65625 -0.375 3.65625 -0.5625 C 3.65625 -0.8125 3.671875 -0.875 3.703125 -1.046875 Z M 3.078125 -1.1875 C 3.015625 -1 3.015625 -0.984375 2.875 -0.8125 C 2.4375 -0.265625 2.03125 -0.109375 1.75 -0.109375 C 1.25 -0.109375 1.109375 -0.65625 1.109375 -1.046875 C 1.109375 -1.546875 1.421875 -2.765625 1.65625 -3.234375 C 1.96875 -3.8125 2.40625 -4.1875 2.8125 -4.1875 C 3.453125 -4.1875 3.59375 -3.375 3.59375 -3.3125 C 3.59375 -3.25 3.578125 -3.1875 3.5625 -3.140625 Z M 3.078125 -1.1875 \"/>\n",
       "</symbol>\n",
       "<symbol overflow=\"visible\" id=\"glyph1-0\">\n",
       "<path style=\"stroke:none;\" d=\"\"/>\n",
       "</symbol>\n",
       "<symbol overflow=\"visible\" id=\"glyph1-1\">\n",
       "<path style=\"stroke:none;\" d=\"M 1.90625 -0.53125 C 1.90625 -0.8125 1.671875 -1.0625 1.390625 -1.0625 C 1.09375 -1.0625 0.859375 -0.8125 0.859375 -0.53125 C 0.859375 -0.234375 1.09375 0 1.390625 0 C 1.671875 0 1.90625 -0.234375 1.90625 -0.53125 Z M 1.90625 -0.53125 \"/>\n",
       "</symbol>\n",
       "<symbol overflow=\"visible\" id=\"glyph1-2\">\n",
       "<path style=\"stroke:none;\" d=\"M 3.65625 -3.171875 L 3.65625 -2.84375 C 3.65625 -0.515625 2.625 -0.0625 2.046875 -0.0625 C 1.875 -0.0625 1.328125 -0.078125 1.0625 -0.421875 C 1.5 -0.421875 1.578125 -0.703125 1.578125 -0.875 C 1.578125 -1.1875 1.34375 -1.328125 1.125 -1.328125 C 0.96875 -1.328125 0.671875 -1.25 0.671875 -0.859375 C 0.671875 -0.1875 1.203125 0.21875 2.046875 0.21875 C 3.34375 0.21875 4.5625 -1.140625 4.5625 -3.28125 C 4.5625 -5.96875 3.40625 -6.640625 2.515625 -6.640625 C 1.96875 -6.640625 1.484375 -6.453125 1.0625 -6.015625 C 0.640625 -5.5625 0.421875 -5.140625 0.421875 -4.390625 C 0.421875 -3.15625 1.296875 -2.171875 2.40625 -2.171875 C 3.015625 -2.171875 3.421875 -2.59375 3.65625 -3.171875 Z M 2.421875 -2.40625 C 2.265625 -2.40625 1.796875 -2.40625 1.5 -3.03125 C 1.3125 -3.40625 1.3125 -3.890625 1.3125 -4.390625 C 1.3125 -4.921875 1.3125 -5.390625 1.53125 -5.765625 C 1.796875 -6.265625 2.171875 -6.390625 2.515625 -6.390625 C 2.984375 -6.390625 3.3125 -6.046875 3.484375 -5.609375 C 3.59375 -5.28125 3.640625 -4.65625 3.640625 -4.203125 C 3.640625 -3.375 3.296875 -2.40625 2.421875 -2.40625 Z M 2.421875 -2.40625 \"/>\n",
       "</symbol>\n",
       "<symbol overflow=\"visible\" id=\"glyph1-3\">\n",
       "<path style=\"stroke:none;\" d=\"M 1.625 -4.5625 C 1.171875 -4.859375 1.125 -5.1875 1.125 -5.359375 C 1.125 -5.96875 1.78125 -6.390625 2.484375 -6.390625 C 3.203125 -6.390625 3.84375 -5.875 3.84375 -5.15625 C 3.84375 -4.578125 3.453125 -4.109375 2.859375 -3.765625 Z M 3.078125 -3.609375 C 3.796875 -3.984375 4.28125 -4.5 4.28125 -5.15625 C 4.28125 -6.078125 3.40625 -6.640625 2.5 -6.640625 C 1.5 -6.640625 0.6875 -5.90625 0.6875 -4.96875 C 0.6875 -4.796875 0.703125 -4.34375 1.125 -3.875 C 1.234375 -3.765625 1.609375 -3.515625 1.859375 -3.34375 C 1.28125 -3.046875 0.421875 -2.5 0.421875 -1.5 C 0.421875 -0.453125 1.4375 0.21875 2.484375 0.21875 C 3.609375 0.21875 4.5625 -0.609375 4.5625 -1.671875 C 4.5625 -2.03125 4.453125 -2.484375 4.0625 -2.90625 C 3.875 -3.109375 3.71875 -3.203125 3.078125 -3.609375 Z M 2.078125 -3.1875 L 3.3125 -2.40625 C 3.59375 -2.21875 4.0625 -1.921875 4.0625 -1.3125 C 4.0625 -0.578125 3.3125 -0.0625 2.5 -0.0625 C 1.640625 -0.0625 0.921875 -0.671875 0.921875 -1.5 C 0.921875 -2.078125 1.234375 -2.71875 2.078125 -3.1875 Z M 2.078125 -3.1875 \"/>\n",
       "</symbol>\n",
       "<symbol overflow=\"visible\" id=\"glyph1-4\">\n",
       "<path style=\"stroke:none;\" d=\"M 4.578125 -3.1875 C 4.578125 -3.984375 4.53125 -4.78125 4.1875 -5.515625 C 3.734375 -6.484375 2.90625 -6.640625 2.5 -6.640625 C 1.890625 -6.640625 1.171875 -6.375 0.75 -5.453125 C 0.4375 -4.765625 0.390625 -3.984375 0.390625 -3.1875 C 0.390625 -2.4375 0.421875 -1.546875 0.84375 -0.78125 C 1.265625 0.015625 2 0.21875 2.484375 0.21875 C 3.015625 0.21875 3.78125 0.015625 4.21875 -0.9375 C 4.53125 -1.625 4.578125 -2.40625 4.578125 -3.1875 Z M 2.484375 0 C 2.09375 0 1.5 -0.25 1.328125 -1.203125 C 1.21875 -1.796875 1.21875 -2.71875 1.21875 -3.3125 C 1.21875 -3.953125 1.21875 -4.609375 1.296875 -5.140625 C 1.484375 -6.328125 2.234375 -6.421875 2.484375 -6.421875 C 2.8125 -6.421875 3.46875 -6.234375 3.65625 -5.25 C 3.765625 -4.6875 3.765625 -3.9375 3.765625 -3.3125 C 3.765625 -2.5625 3.765625 -1.890625 3.65625 -1.25 C 3.5 -0.296875 2.9375 0 2.484375 0 Z M 2.484375 0 \"/>\n",
       "</symbol>\n",
       "<symbol overflow=\"visible\" id=\"glyph1-5\">\n",
       "<path style=\"stroke:none;\" d=\"M 1.265625 -0.765625 L 2.328125 -1.796875 C 3.875 -3.171875 4.46875 -3.703125 4.46875 -4.703125 C 4.46875 -5.84375 3.578125 -6.640625 2.359375 -6.640625 C 1.234375 -6.640625 0.5 -5.71875 0.5 -4.828125 C 0.5 -4.28125 1 -4.28125 1.03125 -4.28125 C 1.203125 -4.28125 1.546875 -4.390625 1.546875 -4.8125 C 1.546875 -5.0625 1.359375 -5.328125 1.015625 -5.328125 C 0.9375 -5.328125 0.921875 -5.328125 0.890625 -5.3125 C 1.109375 -5.96875 1.65625 -6.328125 2.234375 -6.328125 C 3.140625 -6.328125 3.5625 -5.515625 3.5625 -4.703125 C 3.5625 -3.90625 3.078125 -3.125 2.515625 -2.5 L 0.609375 -0.375 C 0.5 -0.265625 0.5 -0.234375 0.5 0 L 4.203125 0 L 4.46875 -1.734375 L 4.234375 -1.734375 C 4.171875 -1.4375 4.109375 -1 4 -0.84375 C 3.9375 -0.765625 3.28125 -0.765625 3.0625 -0.765625 Z M 1.265625 -0.765625 \"/>\n",
       "</symbol>\n",
       "</g>\n",
       "<clipPath id=\"clip1\">\n",
       "  <path d=\"M 14 10 L 41 10 L 41 32.882812 L 14 32.882812 Z M 14 10 \"/>\n",
       "</clipPath>\n",
       "<clipPath id=\"clip2\">\n",
       "  <path d=\"M 69 9 L 93.484375 9 L 93.484375 32.882812 L 69 32.882812 Z M 69 9 \"/>\n",
       "</clipPath>\n",
       "<clipPath id=\"clip3\">\n",
       "  <path d=\"M 3 15 L 26 15 L 26 32.882812 L 3 32.882812 Z M 3 15 \"/>\n",
       "</clipPath>\n",
       "</defs>\n",
       "<g id=\"surface1\">\n",
       "<g clip-path=\"url(#clip1)\" clip-rule=\"nonzero\">\n",
       "<path style=\"fill:none;stroke-width:0.3985;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 7.703687 -0.00140625 C 7.703687 4.2525 4.254469 7.701719 0.0005625 7.701719 C -4.253344 7.701719 -7.702563 4.2525 -7.702563 -0.00140625 C -7.702563 -4.255313 -4.253344 -7.704531 0.0005625 -7.704531 C 4.254469 -7.704531 7.703687 -4.255313 7.703687 -0.00140625 Z M 7.703687 -0.00140625 \" transform=\"matrix(1,0,0,-1,27.601,23.69)\"/>\n",
       "</g>\n",
       "<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n",
       "  <use xlink:href=\"#glyph0-1\" x=\"25.445\" y=\"25.835\"/>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2)\" clip-rule=\"nonzero\">\n",
       "<path style=\"fill:none;stroke-width:0.3985;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 65.688063 -0.00140625 C 65.688063 4.967344 61.660719 8.994687 56.691969 8.994687 C 51.727125 8.994687 47.699781 4.967344 47.699781 -0.00140625 C 47.699781 -4.96625 51.727125 -8.993594 56.691969 -8.993594 C 61.660719 -8.993594 65.688063 -4.96625 65.688063 -0.00140625 Z M 65.688063 -0.00140625 \" transform=\"matrix(1,0,0,-1,27.601,23.69)\"/>\n",
       "</g>\n",
       "<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n",
       "  <use xlink:href=\"#glyph0-2\" x=\"81.701\" y=\"27.149\"/>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip3)\" clip-rule=\"nonzero\">\n",
       "<path style=\"fill:none;stroke-width:0.3985;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -7.647875 -2.048281 C -21.339281 -5.71625 -21.339281 5.717344 -9.054125 2.428281 \" transform=\"matrix(1,0,0,-1,27.601,23.69)\"/>\n",
       "</g>\n",
       "<path style=\"fill:none;stroke-width:0.31879;stroke-linecap:round;stroke-linejoin:round;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -1.196565 1.59439 C -1.096661 0.995557 -0.000663978 0.0985504 0.299805 0.00146589 C -0.000179431 -0.0990903 -1.095311 -0.997931 -1.194056 -1.593483 \" transform=\"matrix(0.96782,0.25931,0.25931,-0.96782,18.5454,21.26352)\"/>\n",
       "<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n",
       "  <use xlink:href=\"#glyph1-1\" x=\"3.321\" y=\"20.17\"/>\n",
       "  <use xlink:href=\"#glyph1-2\" x=\"6.08861\" y=\"20.17\"/>\n",
       "  <use xlink:href=\"#glyph1-3\" x=\"11.06991\" y=\"20.17\"/>\n",
       "</g>\n",
       "<path style=\"fill:none;stroke-width:0.3985;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 6.863844 3.963437 C 20.926344 12.373594 34.395094 12.580625 48.30525 4.834531 \" transform=\"matrix(1,0,0,-1,27.601,23.69)\"/>\n",
       "<path style=\"fill:none;stroke-width:0.31879;stroke-linecap:round;stroke-linejoin:round;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -1.196788 1.5943 C -1.095128 0.995588 0.00136818 0.099503 0.296916 0.00109563 C -0.000306613 -0.097585 -1.095207 -0.997213 -1.195933 -1.592744 \" transform=\"matrix(0.87622,0.48805,0.48805,-0.87622,75.90727,18.85605)\"/>\n",
       "<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n",
       "  <use xlink:href=\"#glyph1-1\" x=\"48.927\" y=\"9.741\"/>\n",
       "  <use xlink:href=\"#glyph1-4\" x=\"51.69461\" y=\"9.741\"/>\n",
       "  <use xlink:href=\"#glyph1-5\" x=\"56.67591\" y=\"9.741\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ]
     },
     "metadata": {
      "isolated": "true"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%tikz -f svg\n",
    "\n",
    "\\usetikzlibrary{shapes}\n",
    "\n",
    "\\node[shape=circle,draw=black] (c) at (0,0) {$c$};\n",
    "\\node[shape=circle,draw=black] (d) at (2,0) {$d$};\n",
    "\n",
    "\\path [->] (c) edge [loop left] node[above] {.98} (c);\n",
    "\\path [->] (c) edge [bend left] node[above] {.02} (d);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other variations\n",
    "\n",
    "There are other ways to vary the way dynamic problems get defined. New variations might either (a) allow for a more parsimonious expression of problems in a more restricted class, or (b) allow for the expression of entirely new kinds of problems.\n",
    "\n",
    "#### Endogenous Gridpoints Method\n",
    "\n",
    "Used to avoid computationally expensive rootfinding operations in the discovery of an optimal action in a continuous space.\n",
    "\n",
    "Carroll, 2006. White, 2015.\n",
    "\n",
    "**TODO**: Tie this into the preceding analysis. In particular, the pre/post-state distinction connects to the preceding work....how?\n",
    "\n",
    "#### Irrational expectations\n",
    "\n",
    "There are some models where we would like agents to have irrational expectations. For example, if they have incorrect views about the stock market's risky returns.\n",
    "\n",
    "A simple approach to the problem is to solve the problem with one set of parameters and then forward simulated using that policy in a problem with new parameters.\n",
    "\n",
    "#### Recursive preferences\n",
    "\n",
    "An example of a class of problems that are not additivitely separable would be recursive preferences. See for instance Section 3, \"the time aggregator\" [here](http://w4.stern.nyu.edu/economics/docs/workingpapers/2005/BRZ%20palgrave.pdf).\n",
    "\n",
    "#### Endogenous information flow\n",
    "\n",
    "A more complex and interesting way to address this is to have endogenous limitations of information flows within the model.\n",
    "\n",
    "#### Strategic agents\n",
    "\n",
    "Multiple interacting agents with different reward functions. Game theoretic or evolutionary equilibria. Build on Koller and Milch (200X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizing the relationship between subproblems of Portfolio Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Modeling Desiderata\n",
    "\n",
    "Once we have all of the information necessary to define a problem, we may be inclined to reshape it into a more tractable form. In general, we would like our models to be easy to understand and easy to solve. These two desiderata may be at odds with each other and can be addressed separately.\n",
    "\n",
    "* D1. Minimal computational complexity of solution\n",
    "* D2. Minimal programmatic description.\n",
    "\n",
    "#### D1. Minimal computational complexity of solution\n",
    "\n",
    "The main technique used to reduce the computational complexity of solving the dynamic optimization is by reducing the size of a problem by decomposing it into equivalent subproblems.\n",
    "\n",
    "For example, it is sometimes possible to reduce a two-dimensional state space into a single dimension where one state dimension is normalized by the other.\n",
    "\n",
    "\n",
    "##### Action space reduction\n",
    "\n",
    "If the action space of the original problem has 2 dimensions, but the problem can be decomposed into two problems with 1 dimensional action spaces, this reduces the complexity of the problem from (roughly speaking) $O(n^2)$ to $O(n)$, a significant reduction.\n",
    "\n",
    "##### State space reduction\n",
    "\n",
    "If the original problem has a state space of 5 dimensions, if the problem can be reduced to 2 dimensions, that reduces the time and space complexity of the problem.\n",
    "\n",
    "Hence, wherever possible, it is beneficial to decompose a repeated problem with a large action space into Carroll subproblems.\n",
    "\n",
    "#### D2. Minimal programmatic description\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dolo in depth\n",
    "\n",
    "\n",
    "_Dolo is always changing. In defining the scope of problems that Dolo solves, we must specify a version number_.\n",
    "\n",
    "[Old Dolo Manual](https://buildmedia.readthedocs.org/media/pdf/dolo/doc/dolo.pdf)\n",
    "\n",
    "\n",
    "### The model object\n",
    "\n",
    "\n",
    "Dolo supports the definition of problems using the following constructs:\n",
    "\n",
    "* [symbols](https://dolo.readthedocs.io/en/latest/model_api.html#symbols): the definition of mathematical symbols. These correspond to:\n",
    "  * dimensions of the action space $A$\n",
    "  * dimensions of the [state space](https://dolo.readthedocs.io/en/latest/model_specification.html#state-space) $X$\n",
    "  * other parameters used\n",
    "* [calibration](https://dolo.readthedocs.io/en/latest/model_api.html#calibration): a dictionary of parameters giving values to symbols.\n",
    "* [functions](https://dolo.readthedocs.io/en/latest/model_api.html#functions): contains several functions that define other aspects of the problem\n",
    "* domain: Domain\n",
    "* [exogenous](https://dolo.readthedocs.io/en/latest/model_api.html#exogenous-shock): A driving Process that changes over time\n",
    "\n",
    "The domain of the exogenous state is implicit in the processes used. These are never used.\n",
    "\n",
    "The endogenous state boundaries are given in a separate part of the YAMl syntax:\n",
    "\n",
    "```\n",
    "domain: \n",
    " - x : [min,max]\n",
    "\n",
    "```\n",
    "\n",
    "The most informative aspect of the model is contained in the `functions` dictionary, which should be described in greater detail.\n",
    "\n",
    "#### Dolo functions\n",
    "\n",
    "Within the `functions` dictionary a modeler can put a number of named functions. These functions are defined over variables in the state, action, and parameters space enumerated in the `symbols` dictionary. We will note here how each element corresponds to an aspect of the Bellman equation formalism.\n",
    "\n",
    "- `felicity`. The reward function per time period, without discount, in the Bellman form value function.\n",
    "   - state, control, parameters -> [vector of reals]^1\n",
    "   - corresponds to $F$\n",
    "- `value`. Thereward/felicity plus the discounted value of the next period (recursive).\n",
    "   - state, control, parameters -> [vector of reals]^1\n",
    "   - corresponds to $V$\n",
    "- `transition` The transistions of the endogenous states as a function of their value in the previous time period, exogenous, and control.\n",
    "   - exogenous_state,  endogenous_state, controls, exogenous_state_today, parameters -> endogenous_state\n",
    "   - corresponds to $T$\n",
    "- `arbitrage`. The first order conditions of the maximization conditions of the problem, the Euler equations. A nonlinear system of equations that needs to be 0 at the maximum (or the minimum, which we hope it's not) or a boundary. There are as many equations as there are control variables.\n",
    "   - endo_state, exo_state, controls, Tomorrow endo_state, Tomorrow_exo_state, Tomorrow_controls, parameters -> [vector of size of controls]\n",
    "   -  Can contain information about the boundaries for the controls. Complementarity conditions. Constraints. I.e. `0 <= n < inf`. This can be replaced with `controls_lb`.\n",
    "   - corresponds to a _solution_ to $V$ **(???)**\n",
    "   - ... and possibly the contraints object $\\Gamma$\n",
    "- `expectation`. **Specifically for Parameterized Expecation method**. An expecation of the future.\n",
    "   - exo_state, endo_state_tomorow, controls_tomorrow -> [size of expectations vector]\n",
    "   - size of output is set in the symbols dictionary\n",
    "   - corresponds to the expectations clause $\\mathbb{E}\\[V(T(x,a)\\]$ in the value function **(???)**\n",
    "- `direct_response`. The optimal control response given expectations of future actions and state today.\n",
    "   - exo_state, endo_state, paramaters  -> controls\n",
    "   - corresponds to a _solution_ to $V$ **(???)**\n",
    "- `controls_lb` and `controls_ub`. Lower and upper bounds for the controls.\n",
    "   - endo_state, exo_state  -> a vector of the lower/upper bounds of each control\n",
    "   - corresponds to the contraints object $\\Gamma$\n",
    "- `terminal_conditions` : in case the model is finite, controls in the last period. [link](https://dolo.readthedocs.io/en/latest/model_specification.html#terminal-conditions)\n",
    "   \n",
    "\n",
    "\n",
    "### Solution algorithms\n",
    "\n",
    "Dolo supports a variety of solution algorithms. These algorithms require a model object as designed above, along with a subset of the allowable functions. The algorithms have different function requirements as follows:\n",
    "\n",
    "* Time iteration: `transition` and `arbitrage`\n",
    "* Improved time iteration: `transition` and `arbitrage`\n",
    "* Perfect foresight: `transition` and `arbitrage`\n",
    "* Perturbation : `transition` and `arbitrage`\n",
    "* Steady state : `transition` and `arbitrage`\n",
    "* Value function iteration : `transition`, and either a `felicity` or `value`\n",
    "* Parameterized expectations : `transition`, `expectation`, and `direct response`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roadmap\n",
    "\n",
    "### Prior Work\n",
    "\n",
    "#### DYNARE\n",
    "\n",
    "The class of problems it can solve:\n",
    " - perfect foresight\n",
    " - indentical problems in time\n",
    " - smoothness\n",
    " \n",
    "\n",
    "### HARK 1.0\n",
    "\n",
    "Herein we define the class of problems that we would like to support as of HARK 1.0.\n",
    "\n",
    "We will define this in terms of the formal definitions listed above.\n",
    "\n",
    "#### Shocks, State, Control, and Reward variables\n",
    "\n",
    "\n",
    "\n",
    "Here, there are threes kinds of variables: shocks ($\\phi$, $\\psi$, ...), state ($a$, $b$, ...)  and control ($c$ and $\\PortShare$).\n",
    "\n",
    "There is also another kind of value, a reward or utility value that in this case is dependent on a control variable.\n",
    "\n",
    "#### Finite Horizon iteration\n",
    "\n",
    "Repeat a problem a finite number of time $T$.\n",
    "\n",
    "\n",
    "#### Backwards induction\n",
    "\n",
    "For each control variable, compute the optimal policy.\n",
    "\n",
    "This is done through the calculation of the Bellman equation:\n",
    "\n",
    "$$V(x) = \\text{max}_{a \\in \\Gamma(a)} F(x,a) + \\beta V(T(x,a))  $$\n",
    "\n",
    "Where $\\Gamma$ are constraints on the actions available and $T$ is the transition function to the next state.\n",
    "\n",
    "Note in this formulation $a$ and $x$ are considered in terms of the broader classes of State and Action \n",
    "\n",
    "Compute this with a time adjusted value function \n",
    "\n",
    "\n",
    "\n",
    "#### Market equilibria\n",
    "\n",
    "Multiple agents interacting with a Market, with rational expectations of the behavior of the market.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HARK 2.0\n",
    "\n",
    "Herein we define the class of problems that we would like to support as of HARK 2.0.\n",
    "\n",
    "We will define this in terms of the formal definitions listed above.\n",
    "\n",
    "We anticipate combining the scope of Dolo and HARK 1.0 into HARK 2.0\n",
    "\n",
    "#### Arbitrary Problem Embeddings\n",
    "\n",
    "Let a problem be defined as as set containing:\n",
    " - \n",
    " - \n",
    " - \n",
    " - \n",
    " - \n",
    " - \n",
    " \n",
    "(This is all the information included in a Dolang document).\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior work\n",
    "\n",
    "### Modeling different kinds of variables\n",
    "\n",
    "Koller and Milch (2003) take the approach in their Multi-Agent Influence Diagrams (MAIDs) of using:\n",
    " - ovals for Chance variables or \"decisions by nature\"\n",
    " - rectangles for Decision variables (roughly equivalent to Control variables here)\n",
    " - diamonds for Utility variables, which correspond to utility gained by the agents.\n",
    " \n",
    " We can adapt this to the portfolio choice model as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "There exist a number of tools that are designed for flexible notation for visualizing [probabilistic graphical models](http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture4.pdf) (see also [Koller and Friedman, 2009](https://djsaunde.github.io/read/books/pdfs/probabilistic%20graphical%20models.pdf))\n",
    "\n",
    "\n",
    "### tikz BayesNet\n",
    "\n",
    "https://github.com/jluttine/tikz-bayesnet\n",
    "\n",
    "* LaTeX, builds on [Tikz](https://es.overleaf.com/learn/latex/TikZ_package)\n",
    "* Plate notation\n",
    "* Can handle factor graphs\n",
    "\n",
    "### Daft\n",
    "\n",
    "https://docs.daft-pgm.org/en/latest/\n",
    "\n",
    "* Python based, used `matplotlib`\n",
    "* Programmatic graph definition is nice\n",
    "* built-in distinction between 'observed' and 'unobserved' variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### GraphViz\n",
    "\n",
    "https://stackoverflow.com/a/16334517\n",
    "\n",
    "* Most flexible\n",
    "* not LaTeX native\n",
    "* Requires additional C dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"390pt\" height=\"116pt\"\n",
       " viewBox=\"0.00 0.00 389.98 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-112 385.9839,-112 385.9839,4 -4,4\"/>\n",
       "<!-- A -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>A</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"190.9919\" cy=\"-90\" rx=\"53.8905\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"190.9919\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">King Arthur</text>\n",
       "</g>\n",
       "<!-- B -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>B</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"90.9919\" cy=\"-18\" rx=\"90.9839\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"90.9919\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Sir Bedevere the Wise</text>\n",
       "</g>\n",
       "<!-- A&#45;&gt;B -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>A&#45;&gt;B</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M168.3004,-73.6621C155.1015,-64.1589 138.2402,-52.0188 123.6348,-41.5029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"125.5012,-38.5339 115.3408,-35.5312 121.4111,-44.2146 125.5012,-38.5339\"/>\n",
       "</g>\n",
       "<!-- L -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>L</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"290.9919\" cy=\"-18\" rx=\"90.9839\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"290.9919\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Sir Lancelot the Brave</text>\n",
       "</g>\n",
       "<!-- A&#45;&gt;L -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>A&#45;&gt;L</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M213.6835,-73.6621C226.8824,-64.1589 243.7437,-52.0188 258.3491,-41.5029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"260.5728,-44.2146 266.6431,-35.5312 256.4826,-38.5339 260.5728,-44.2146\"/>\n",
       "</g>\n",
       "<!-- B&#45;&gt;L -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>B&#45;&gt;L</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M182.0076,-18C184.6151,-18 187.2226,-18 189.8302,-18\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"189.8878,-21.5001 199.8878,-18 189.8878,-14.5001 189.8878,-21.5001\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fc39821bdc0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz as gv\n",
    "\n",
    "dot = gv.Digraph(comment='The Round Table')\n",
    "\n",
    "dot.node('A', 'King Arthur')\n",
    "dot.node('B', 'Sir Bedevere the Wise')\n",
    "dot.node('L', 'Sir Lancelot the Brave')\n",
    "\n",
    "dot.edges(['AB', 'AL'])\n",
    "dot.edge('B', 'L', constraint='false')\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Carroll, C. D. (2006). The method of endogenous gridpoints for solving dynamic stochastic optimization problems. Economics letters, 91(3), 312-320.\n",
    "\n",
    "Koller, D., & Milch, B. (2003). Multi-agent influence diagrams for representing and solving games. Games and economic behavior, 45(1), 181-221.\n",
    "\n",
    "Koller, D., & Friedman, N. (2009). Probabilistic graphical models: principles and techniques. MIT press.\n",
    "\n",
    "Powell, W. B. (2007). Approximate Dynamic Programming: Solving the curses of dimensionality (Vol. 703). John Wiley & Sons.\n",
    "\n",
    "White, M. N. (2015). The method of endogenous gridpoints in theory and practice. Journal of Economic Dynamics and Control, 60, 26-41.\n",
    "\n",
    "### quantecon on time iteration\n",
    "\n",
    "Wilbur John Coleman. Solving the Stochastic Growth Model by Policy-Function Iteration. Journal of Business & Economic Statistics, 8(1):2729, 1990.\n",
    "\n",
    "https://julia.quantecon.org/dynamic_programming/coleman_policy_iter.html\n",
    "\n",
    "### llorracc solving micro dsops\n",
    "https://llorracc.github.io/SolvingMicroDSOPs\n",
    "\n",
    "building on Carrolll EGM\n",
    "\n",
    "Kabukuolu, A., & Martnez-Garca, E. (2020). A Generalized Time Iteration Method for Solving Dynamic Optimization Problems with Occasionally Binding Constraints. Computational Economics, 1-26.\n",
    "\n",
    "\n",
    "### other\n",
    "\n",
    "http://people.duke.edu/~acb8/notes1.pdf\n",
    "\n",
    "http://w4.stern.nyu.edu/economics/docs/workingpapers/2005/BRZ%20palgrave.pdf\n",
    "\n",
    "https://www.sas.upenn.edu/~jesusfv/Lecture_SM_1_VFI.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "econ-ark-3.8",
   "language": "python",
   "name": "econ-ark-3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
