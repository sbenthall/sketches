{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frontier Notation\n",
    "\n",
    "This notebook considers the design and implementation of the next generation of heterogenous agent model (HAM) notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "### Bellman Equation\n",
    "\n",
    "A [Bellman equation](https://en.wikipedia.org/wiki/Bellman_equation) is a formulation of the value to an agent of each state acting optimally within a dynamic decision problem. From this value function it is easy to compute the optimal policy for the agent. The Bellman equation takes the form (**Definition 1**):\n",
    "\n",
    "$$V(x) = \\text{max}_{a \\in \\Gamma(x)} F(x,a) + \\beta V(T(x,a))$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $a \\in A$, the set of actions available\n",
    "* $x \\in X$, the set of states\n",
    "* $\\Gamma : X \\rightarrow \\mathscr{B}(A)$ constraints the actions available in state X\n",
    "* $F(x,a)$ is the payoff to the agent of taking action $a$ in state $x$\n",
    "* $T: X \\times A \\rightarrow X$ is a transition function determining the state in the next period\n",
    "* $\\beta$ is a discount factor on future payoffs.\n",
    "\n",
    "This representation of Bellman equation assumes a deterministic model. If T is a stochastic function, as it is in many cases in economics, then the value function is defined in terms of expectations. (**Definition 2**):\n",
    "\n",
    "$$V(x) = \\text{max}_{a \\in \\Gamma(x)} F(x,a) + \\beta E[V(T(x,a))]$$\n",
    "\n",
    "We will use the deterministic version of this equation for simplicity unless there's a good reason to use the probabilistic version.\n",
    "\n",
    "Note that with this rendition of the problem, there is only one state space, action space, value function, etc. that is repeated \"over time\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variations\n",
    "\n",
    "The above formulation of the Bellman equation assumes that the problem is structured identically at every control point (opportunity for the agent to choose an action $a$) and that there is only one control point. For various applications in economics, we are interested in other kinds of problems, such as those with:\n",
    "\n",
    "* **V1**. Multiple control points\n",
    "* **V2**. Inconsistent problems over time\n",
    "\n",
    "While it may be tempting to consider these the same kind of challenge, we will examine each case rigorously in isolation and see if that conclusion can be confirmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V1. Multiple control points\n",
    "\n",
    "**V1** (Variation 1) can be understood as a decomposition of the problem described in **Definition 1**. For reasons connected to those described in \"Mathematics of Discrete Time Stochastic Control Processes in Macroeconomics\", problems of the following form, for $C$ (sequential) control variables, can be shown to have an equivalent problem in the above form: (Note that the variables here are in italic font)\n",
    "\n",
    "* $(\\mathit{A}_0, ..., \\mathit{A}_C)$ is a sequence of sets of action spaces such that $A = \\mathit{A}_0 \\times ... \\times \\mathit{A}_c$\n",
    "* $(\\mathit{X}_0, ...,\\mathit{X}_C)$ is a sequence of sets of state spaces such that $X = \\mathit{X}_0 \\times ... \\times \\mathit{X}_c$\n",
    "* $(\\mathit{\\Gamma}_0, ..., \\mathit{\\Gamma}_C)$ is a sequence of contraints. $\\mathit{\\Gamma}_c: \\mathit{X}_c \\rightarrow \\mathscr{B}(\\mathit{A}_c)$\n",
    "* $(\\mathit{F}_0, ..., \\mathit{F}_C)$ is a sequence reward functions. $\\mathit{F}_c: \\mathit{X}_c \\times \\mathit{A}_c \\rightarrow \\mathscr{R}$\n",
    "* $(\\mathit{T}_0, ..., \\mathit{T}_C)$ is a sequence of transitions functions $\\mathit{T}_c: \\mathit{X}_c \\times \\mathit{A}_c \\rightarrow \\mathit{X}_{(c+1) mod C}$. **Note incrementing of state space in transition function.**\n",
    "* *Unknown:* What happens to $\\beta$ the discount factor?\n",
    "\n",
    "With these sequences of subproblems defined, the decomposed into $C$ mutually recursive Bellman-like equation for $x \\in \\mathit{X}_c$ is:\n",
    "\n",
    "$$\\mathit{V}_c(x) = \\text{max}_{a \\in \\mathit{\\Gamma}_c(x)} \\mathit{F}_c(x,a) + \\beta \\mathit{V}_{(c+1) mod C}(\\mathit{T}_c(x,a))$$\n",
    "\n",
    "In principle, the original problem could be defined in such a way that the sub-problems need not have a natural sequential order. However, in these cases multiple ordering will suffice.\n",
    "\n",
    "Note that these subproblems are not isolated from each other. The transition function $\\mathit{T}_c$ of each problem contains a \"reference\" to the state space in the next \"stage\" $\\mathit{X}_{c+1}$.\n",
    "\n",
    "These truly are \"subproblems\" of the original problem specification, as the original specification can be reconstituted from the details of the subproblems. For example, $A = \\mathit{A}_0 \\times ... \\times \\mathit{A}_c$, $X = \\mathit{X}_0 \\times ... \\times \\mathit{X}_c$ and so on. The problem remains an infinite horizon problem, but decomposed into $C$ simpler sub-problems with reduced action and state spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V2. Inconsistent problems over time\n",
    "\n",
    "There is a sense in which the challenge of inconsistency over time is similar to the challenge of multiple control points. We will see that the formal descriptions of the challenges are similar. However, there are some differences:\n",
    "\n",
    "1. Whereas multiple control points allow us to break a problem down into $C$ subproblems with smaller state and action spaces than the original problem, inconsistency over time makes a problem *more* complex because it *increases* the size of the problem.\n",
    "2. One of these things will hold for any actually specified inconsistent problem: (a) the inconsistent problem is a finite problem, or (b) the inconsistent problem can be rewritten as a consistent problem with time index in its state space.\n",
    "\n",
    "We will formally define the time inconsistent problem in terms of the following, assuming for now that it is a finite problem:\n",
    "\n",
    "* $A_0, ..., A_K$ is a sequence of sets of action spaces\n",
    "* $X_0, ...,X_K$ is a sequence of sets of state spaces\n",
    "* $\\Gamma_0, ..., \\Gamma_C$ is a sequence of contraints. $\\Gamma_k: X_k \\rightarrow \\mathscr{B}(A_k)$\n",
    "* $F_0, ..., F_K$ is a sequence reward functions. $F_k: X_k \\times A_k \\rightarrow \\mathscr{R}$\n",
    "* $T_0, ..., T_K$ is a sequence of transitions functions $T_k: X_k \\times A_k \\rightarrow X_{k+1}$.\n",
    "* $\\beta$, a discount factor for utility between time steps **Or does this vary?***\n",
    "\n",
    "With these sequences of subproblems defined, the Bellman-like equations are:\n",
    "\n",
    "$$V_K(x) = \\text{max}_{a \\in \\Gamma_K(x)} F_K(x,a)$$\n",
    "$$V_k(x) = \\text{max}_{a \\in \\Gamma_k(x)} F_k(x,a) + \\beta V_{k+1}(T_k(x,a))$$\n",
    "\n",
    "Because this is a finite model, this can be solved simply via backwards induction without looking for a convergent value of $V_k$. Note that as before, the transition functions $T_k$ link the problem at $k$ to the state space of the next time $X_{k+1}$.\n",
    "\n",
    "If the problem for any $k$ carries as much information as one problem in the mode of **Definition 1**, then the time inconsistent problem will have a longer minimum description lengtht than the original infinite horizon problem. This is in contrast to the decomposition of the infinite horizon problem in to multiple stages based on multiple control variables; in the latter case, the decomposition contains no new information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Modeling Desiderata\n",
    "\n",
    "Once we have all of the information necessary to define a problem, we may be inclined to reshape it into a more tractable form. In general, we would like our models to be easy to understand and easy to solve. These two desiderata may be at odds with each other and can be addressed separately.\n",
    "\n",
    "* D1. Minimal programmatic description.\n",
    "* D2. Minimal computational complexity of solution\n",
    "\n",
    "#### D1. Minimal programmatic description\n",
    "\n",
    "#### D2. Minimal computational complexity of solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roadmap\n",
    "\n",
    "### HARK 1.0\n",
    "\n",
    "#### Shocks, State, Control, and Reward variables\n",
    "\n",
    "Consider this version of the portfolio choice consumption saving problem.\n",
    "\n",
    "Variable | Equation | Operation | Stage | Utility\n",
    "-- | -- | -- | -- | --\n",
    "\\Risky | ~Dist | Shock | c | --\n",
    "\\TranShk | ~Dist | Shock | c | --\n",
    "\\PermShk | ~Dist | Shock | c | --\n",
    "\\Rport | \\Rport = \\PortShare * \\Risky + (1 - \\PortShare) * R | Update | c | --\n",
    "$b$ | b_{t} = a_{t-1} \\RPort | Update | c | --\n",
    "$p$ | p_{t}=p_{t-1}\\PermShk_{t} | Update | c | --\n",
    "$y$ | y_{t} = p_{t}\\TranShk_{t} | Update | c | --\n",
    "$m$ | m_{t} = b_{t} + y_{t} | Update | c | --\n",
    "$c$ | c | Control | c | U(c)\n",
    "$a$ | a_{t} = m_{t} - c_{t} | Update | $\\alpha$ | --\n",
    "$\\alpha$| $\\alpha$ | Control | $\\alpha$ | 0\n",
    "\n",
    "Here, there are threes kinds of variables: shocks ($\\phi$, $\\psi$, ...), state ($a$, $b$, ...)  and control ($c$ and $\\PortShare$).\n",
    "\n",
    "There is also another kind of value, a reward or utility value that in this case is dependent on a control variable.\n",
    "\n",
    "#### Finite Horizon iteration\n",
    "\n",
    "Repeat a problem a finite number of time $T$.\n",
    "\n",
    "\n",
    "#### Backwards induction\n",
    "\n",
    "For each control variable, compute the optimal policy.\n",
    "\n",
    "This is done through the calculation of the Bellman equation:\n",
    "\n",
    "$$V(x) = \\text{max}_{a \\in \\Gamma(a)} F(x,a) + \\beta V(T(x,a))  $$\n",
    "\n",
    "Where $\\Gamma$ are constraints on the actions available and $T$ is the transition function to the next state.\n",
    "\n",
    "Note in this formulation $a$ and $x$ are considered in terms of the broader classes of State and Action \n",
    "\n",
    "Compute this with a time adjusted value function \n",
    "\n",
    "\n",
    "\n",
    "#### Market equilibria\n",
    "\n",
    "Multiple agents interacting with a Market, with rational expectations of the behavior of the market.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HARK 2.0\n",
    "\n",
    "#### Arbitrary Problem Embeddings\n",
    "\n",
    "Let a problem be defined as as set containing:\n",
    " - \n",
    " - \n",
    " - \n",
    " - \n",
    " - \n",
    " - \n",
    " \n",
    "(This is all the information included in a Dolang document).\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior work\n",
    "\n",
    "### Modeling different kinds of variables\n",
    "\n",
    "Koller and Milch (2003) take the approach in their Multi-Agent Influence Diagrams (MAIDs) of using:\n",
    " - ovals for Chance variables or \"decisions by nature\"\n",
    " - rectangles for Decision variables (roughly equivalent to Control variables here)\n",
    " - diamonds for Utility variables, which correspond to utility gained by the agents.\n",
    " \n",
    " We can adapt this to the portfolio choice model as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "There exist a number of tools that are designed for flexible notation for visualizing [probabilistic graphical models](http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture4.pdf) (see also [Koller and Friedman, 2009](https://djsaunde.github.io/read/books/pdfs/probabilistic%20graphical%20models.pdf))\n",
    "\n",
    "\n",
    "### tikz BayesNet\n",
    "\n",
    "https://github.com/jluttine/tikz-bayesnet\n",
    "\n",
    "* LaTeX, builds on [Tikz](https://es.overleaf.com/learn/latex/TikZ_package)\n",
    "* Plate notation\n",
    "* Can handle factor graphs\n",
    "\n",
    "### Daft\n",
    "\n",
    "https://docs.daft-pgm.org/en/latest/\n",
    "\n",
    "* Python based, used `matplotlib`\n",
    "* Programmatic graph definition is nice\n",
    "* built-in distinction between 'observed' and 'unobserved' variable\n",
    "\n",
    "### GraphViz\n",
    "\n",
    "https://stackoverflow.com/a/16334517\n",
    "\n",
    "* Most flexible \n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\n",
    "Koller, D., & Milch, B. (2003). Multi-agent influence diagrams for representing and solving games. Games and economic behavior, 45(1), 181-221.\n",
    "\n",
    "Koller, D., & Friedman, N. (2009). Probabilistic graphical models: principles and techniques. MIT press."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "econ-ark",
   "language": "python",
   "name": "econ-ark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
